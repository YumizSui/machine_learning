{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMM4Xwcd7zTA"
   },
   "source": [
    "# 演習問題\n",
    "\n",
    "与えられたニュースをカテゴリに仕分けする分類器（カテゴリ分類器）を構築したい．今回は，[livedoor ニュースコーパス](https://www.rondhuit.com/download.html)を用い，記事の本文やタイトルからその情報源を推定する分類器を構築する．\n",
    "\n",
    "+ トピックニュース: http://news.livedoor.com/category/vender/news/\n",
    "+ Sports Watch: http://news.livedoor.com/category/vender/208/\n",
    "+ ITライフハック: http://news.livedoor.com/category/vender/223/\n",
    "+ 家電チャンネル: http://news.livedoor.com/category/vender/kadench/\n",
    "+ MOVIE ENTER: http://news.livedoor.com/category/vender/movie_enter/\n",
    "+ 独女通信: http://news.livedoor.com/category/vender/90/\n",
    "+ エスマックス: http://news.livedoor.com/category/vender/smax/\n",
    "+ livedoor HOMME: http://news.livedoor.com/category/vender/homme/\n",
    "+ Peachy: http://news.livedoor.com/category/vender/ldgirls/\n",
    "\n",
    "\n",
    "なお，ニュースのカテゴリ分類は[ニュース・キュレーションサービスなどで実際に用いられている](https://webtan.impress.co.jp/e/2015/04/14/19666)技術である．今回の演習では，構築する分類器は線形識別器（多クラスロジスティック回帰など）に限定する（多層ニューラルネットワークや非線形サポートベクトルマシンを使ってはいけない）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cJmFQTg7zTG"
   },
   "source": [
    "## 1. データのダウンロードと整形\n",
    "\n",
    "[livedoor ニュースコーパス](https://www.rondhuit.com/download.html)は[クリエイティブ・コモンズライセンス「表示 – 改変禁止」](https://creativecommons.org/licenses/by-nd/2.1/jp/)のライセンスで配布されているため，データを加工したものを再配布することができない．そこで，データのダウンロードから整形まで，各自の環境で実行する必要がある．データの整形を行う手順を以下に示すので，そのまま実行するだけでよい．ただし，この演習では全員が同じ学習データ，検証データ，評価データを用いたいので，以下の手順を改変することなく実行せよ．\n",
    "\n",
    "### 1.1. 訓練データ，検証データ，評価データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GermHoa77zTH"
   },
   "source": [
    "コーパスをダウンロード．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhiq2ygv7zTH",
    "outputId": "e1da6c07-4ca0-42f9-fb71-d5e6c7110751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-26 12:29:57--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "CA証明書 '/etc/ssl/certs/ca-certificates.crt' をロードしました\n",
      "www.rondhuit.com (www.rondhuit.com) をDNSに問いあわせています... 59.106.19.174\n",
      "www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 8855190 (8.4M) [application/x-gzip]\n",
      "`ldcc-20140209.tar.gz' に保存中\n",
      "\n",
      "ldcc-20140209.tar.g 100%[===================>]   8.44M  32.0MB/s 時間 0.3s       \n",
      "\n",
      "2020-12-26 12:29:57 (32.0 MB/s) - `ldcc-20140209.tar.gz' へ保存完了 [8855190/8855190]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN3POqfk7zTJ"
   },
   "source": [
    "ダウンロードしたコーパスを解凍．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pgKnQku87zTK"
   },
   "outputs": [],
   "source": [
    "!tar -zxvf ldcc-20140209.tar.gz > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRXVR7L07zTK"
   },
   "source": [
    "解凍したファイルを読み込み，記事のリストとしてデータ`D`を作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FN3sOJev7zTK"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "D = []\n",
    "p = pathlib.Path('text')\n",
    "for d in p.iterdir():\n",
    "    if not d.is_dir():\n",
    "        continue\n",
    "    source = d.name\n",
    "    for fname in d.glob('*.txt'):\n",
    "        with open(fname) as fi:\n",
    "            url = fi.readline().strip()\n",
    "            timestamp = fi.readline().strip()\n",
    "            title = fi.readline().strip()\n",
    "            text = [line.strip() for line in fi if line.strip()]\n",
    "            D.append(\n",
    "                dict(source=source, url=url, timestamp=timestamp, title=title, text=text)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y08LPtH7zTL"
   },
   "source": [
    "訓練データ`Dtrain`, 検証データ`Ddev`，評価データ`Dtest`に分ける．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PYpQ8ulW7zTL"
   },
   "outputs": [],
   "source": [
    "D.sort(key=lambda x: x['url'])\n",
    "\n",
    "Dtrain = []\n",
    "Ddev = []\n",
    "Dtest = []\n",
    "\n",
    "for i, d in enumerate(D):\n",
    "    if i % 10 == 8:\n",
    "        Ddev.append(d)\n",
    "    elif i % 10 == 9:\n",
    "        Dtest.append(d)\n",
    "    else:\n",
    "        Dtrain.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QPxCRxW7zTM"
   },
   "source": [
    "コーパスを訓練データ，検証データ，評価データに正しく分割できたかを，ハッシュ値を用いてチェックする．もし，以下のコードを実行した時に**AssertionError例外が出た場合はそれまでの手順を変更してしまった可能性がある**（正常であれば\"OK\"と表示される）．どうしてもAssertionError例外が出る場合は連絡すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFNbvx1-7zTM",
    "outputId": "ace69405-5814-4f44-88bc-a60f7f73d72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def compute_hash(D):\n",
    "    m = hashlib.sha256()\n",
    "    for d in D:\n",
    "        m.update(d['url'].encode('utf-8'))\n",
    "    return m.hexdigest()\n",
    "\n",
    "assert compute_hash(Dtrain) == 'f1294a19b25952e5b18510e3eb74c21be9d5d18a86c369d2d2639c9e5ea93d6c'\n",
    "assert compute_hash(Ddev) == '64f709e1e739ac880b8b7acc49ce342b60e80b804279bac68c5f27d08b5fb141'\n",
    "assert compute_hash(Dtest) == '4acf6822099a9e4cc5794cade26ae0ddd8df88ccc99690e7b48cdd8aa3bf1bcd'\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQoWcCPT7zTM"
   },
   "source": [
    "### 1.2 日本語の分かち書き"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZAYAjS77zTN"
   },
   "source": [
    "タイトルと本文の日本語を分かち書きするために，[MeCab](https://taku910.github.io/mecab/)をインストールする（mecab-python3の最新版は環境によってエラーが出るので，v0.996.5を指定してインストールしている）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB4Fd0wv7zTN",
    "outputId": "5431d62f-b2e6-43be-9d4f-73f4a388b72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mecab-python3==0.996.5\n",
      "  Downloading mecab_python3-0.996.5-cp38-cp38-manylinux2010_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mecab-python3\n",
      "Successfully installed mecab-python3-0.996.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3==0.996.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxhiv0Mq7zTN"
   },
   "source": [
    "タイトルと本文を単語（形態素）区切りで分割する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g7TW18hI7zTO"
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "def tokenize(s):\n",
    "    return tagger.parse(s).split()\n",
    "\n",
    "def add_tokenization(D):\n",
    "    for d in D:\n",
    "        d['title.tokenized'] = tokenize(d['title'])\n",
    "        d['text.tokenized'] = [tokenize(s) for s in d['text']]\n",
    "\n",
    "add_tokenization(Dtrain)\n",
    "add_tokenization(Ddev)\n",
    "add_tokenization(Dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhURYrE97zTO"
   },
   "source": [
    "### 整形済みのデータをファイルに保存する場合\n",
    "\n",
    "もし，整形済みのデータをファイルに保存しておきたい場合は，以下のコードを実行して\"livedoor.json\"というファイルに保存する．ただし，Google Colaboratory上で実行している場合は，インスタンスが消滅すると保存したファイルも消えてしまうので，以下のいずれかで対応することになる．\n",
    "\n",
    "1. インスタンスを新たに立ち上げた（インスタンスがリセットされた）度に，これまでの処理を再実行する\n",
    "1. \"livedoor.json\"を自分のPCに保存しておき，インスタンスを立ち上げる毎にアップロードする\n",
    "1. \"livedoor.json\"を自分のGoogle Driveに保存しておき，インスタンスを立ち上げたときにマウントして読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "dSiGf7qi7zTO"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('livedoor.json', 'w') as fo:\n",
    "    json.dump(\n",
    "        dict(train=Dtrain, test=Dtest, dev=Ddev),\n",
    "        fo\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nCBrzGJ7zTP"
   },
   "source": [
    "### 保存された整形済みのデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dhIIhPSI7zTP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('livedoor.json') as fi:\n",
    "    D = json.load(fi)\n",
    "    \n",
    "Dtrain = D['train']\n",
    "Ddev = D['dev']\n",
    "Dtest = D['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUNXbkVc7zTP"
   },
   "source": [
    "## 作成されたデータの確認\n",
    "\n",
    "訓練データの先頭の事例を表示してみる．各訓練データは，以下のような辞書で表現される．各フィールドの意味は以下の通りである．\n",
    "\n",
    "+ `source`: 記事のカテゴリ\n",
    "+ `url`: 記事のURL\n",
    "+ `timestamp`: 記事の発行日時\n",
    "+ `title`: 記事のタイトル\n",
    "+ `text`: 記事の本文（段落（文字列）を要素としたリスト形式\n",
    "+ `title.tokenized`: 記事のタイトルをMeCabで分かち書きしたもの\n",
    "+ `text.tokenized`: 記事の本文を分かち書きしたもの．段落が単語のリストとして表現され，その段落のリストを格納している\n",
    "\n",
    "`source`フィールドのクラスを目的変数とみなし，それ以外のフィールドの情報から目的変数を予測する高性能なモデルを構築するのが，今回の演習の趣旨である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYKSLHoJ7zTP",
    "outputId": "2b437d7f-d425-402c-90c1-97ed7fd7272d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'peachy',\n",
       " 'url': 'http://news.livedoor.com/article/detail/6684605/',\n",
       " 'timestamp': '2000-06-25T15:45:00+0900',\n",
       " 'title': 'キーワードは「カワイク＆賢く！」イマドキスマホ女子に人気のスマホグッズ紹介',\n",
       " 'text': ['・カカオチョコレート',\n",
       "  '・ウサギケース ラビットしっぽ',\n",
       "  '・フォンピアス\\u3000イヤホンジャックアクセサリー\\u3000クマ',\n",
       "  '・ブラウンポンポンゴールド スマートフォンアクセ'],\n",
       " 'title.tokenized': ['キーワード',\n",
       "  'は',\n",
       "  '「',\n",
       "  'カワイク',\n",
       "  '＆',\n",
       "  '賢く',\n",
       "  '！',\n",
       "  '」',\n",
       "  'イマドキスマホ',\n",
       "  '女子',\n",
       "  'に',\n",
       "  '人気',\n",
       "  'の',\n",
       "  'スマホグッズ',\n",
       "  '紹介'],\n",
       " 'text.tokenized': [['・', 'カカオ', 'チョコレート'],\n",
       "  ['・', 'ウサギ', 'ケース', 'ラビット', 'しっぽ'],\n",
       "  ['・', 'フォン', 'ピアス', 'イヤホンジャックアクセサリー', 'クマ'],\n",
       "  ['・', 'ブラウンポンポンゴールド', 'スマートフォンアクセ']]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtrain[4749]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHC2Th5MruNd"
   },
   "source": [
    "## 2. 単語の出現頻度による特徴量ベクトル\n",
    "\n",
    "学習データ`Dtrain`に含まれる任意の事例に対して，分かち書きされたテキスト（`text.tokenized`）に含まれる単語の出現頻度を計測し，単語から頻度への連想配列（辞書）形式のオブジェクトに格納せよ（小レポート1 7-1を参考にせよ）．例として，`Dtrain[3521]`の学習事例のテキストに対して，単語の出現頻度を計測した結果の一部を示す．\n",
    "\n",
    "```\n",
    "{'4': 1,\n",
    " '月': 2,\n",
    " '19': 1,\n",
    " '日': 1,\n",
    " '（': 3,\n",
    " '）': 3,\n",
    " 'より': 1,\n",
    " ...\n",
    " '類': 1,\n",
    " 'と': 1,\n",
    " 'なる': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2vec(token):\n",
    "    vec = defaultdict(int)\n",
    "    for sentence in token:\n",
    "        for word in sentence:\n",
    "                vec[word] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "FfPXZTtzvoij"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'4': 1,\n",
       "             '月': 2,\n",
       "             '19': 1,\n",
       "             '日': 1,\n",
       "             '（': 3,\n",
       "             '）': 3,\n",
       "             'より': 1,\n",
       "             '、': 6,\n",
       "             '神戸': 1,\n",
       "             '大丸': 1,\n",
       "             'インテリア': 1,\n",
       "             '専門': 1,\n",
       "             '館': 1,\n",
       "             '「': 2,\n",
       "             'ミュゼエール': 1,\n",
       "             '＠': 1,\n",
       "             '六甲': 1,\n",
       "             'アイランド': 1,\n",
       "             '」': 2,\n",
       "             '2': 1,\n",
       "             '階': 1,\n",
       "             'で': 2,\n",
       "             'は': 3,\n",
       "             'ヨーロッパ': 1,\n",
       "             '最大': 1,\n",
       "             'の': 2,\n",
       "             'フィットネスマシンメーカー': 1,\n",
       "             'テクノ': 1,\n",
       "             'ジム': 1,\n",
       "             '社': 1,\n",
       "             'パートナー': 1,\n",
       "             'ショールーム': 1,\n",
       "             'が': 1,\n",
       "             '関西': 1,\n",
       "             '初めて': 1,\n",
       "             '開設': 1,\n",
       "             'さ': 1,\n",
       "             'れる': 1,\n",
       "             '。': 2,\n",
       "             '展示': 1,\n",
       "             '品': 1,\n",
       "             'キネシス・パーソナル': 1,\n",
       "             'Vision': 1,\n",
       "             'ユニカ': 1,\n",
       "             'Black': 2,\n",
       "             '／': 1,\n",
       "             'Recline': 1,\n",
       "             'Forma': 1,\n",
       "             'アクセサリー': 1,\n",
       "             '類': 1,\n",
       "             'と': 1,\n",
       "             'なる': 1})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = Dtrain[238]['text.tokenized']\n",
    "token2vec(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nutbuifI7zTQ"
   },
   "source": [
    "## 3. 線形分類モデルの学習\n",
    "\n",
    "2.で構築したプログラムを使い，学習データ`Dtrain`の分かち書きされたテキスト（`text.tokenized`）に含まれる単語の頻度を特徴量ベクトル$\\pmb{x}$として，目的変数（情報源である`source`フィールド）を予測する線形識別モデルを学習せよ．\n",
    "\n",
    "ヒント\n",
    "+ 線形分類モデルの実装に[sklearn.linear_model.SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)を使う場合は，単語とクラス名を自然数のID番号に変換し，学習事例を`np.array`に変換する必要がある．この変換には，小レポート1 7-2が参考になるし，[sklearn.feature_extraction.DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)および[sklearn.preprocessing.LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)を使ってもよい．\n",
    "+ 特徴量の空間，すなわち線形分類モデルが扱うことのできる単語集合は訓練データ中に含まれる全ての単語とすればよい．\n",
    "+ 訓練データ中には出現しなかったが，検証データや評価データのみに出現する単語がある．そのような単語（OOV: out-of-vocabulary）は無視すればよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FqQNuhWV7zTQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2vec(token):\n",
    "    vec = defaultdict(int)\n",
    "    for sentence in token:\n",
    "        for word in sentence:\n",
    "                vec[word] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = [token2vec(data['text.tokenized']) for data in Dtrain]\n",
    "dev_vec = [token2vec(data['text.tokenized']) for data in Ddev]\n",
    "test_vec = [token2vec(data['text.tokenized']) for data in Dtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to IDVec\n",
    "VX = DictVectorizer()\n",
    "Xtrain = VX.fit_transform(train_vec).toarray()\n",
    "Xdev = VX.transform(dev_vec).toarray()\n",
    "Xtest = VX.transform(test_vec).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source to ID\n",
    "VY = preprocessing.LabelEncoder()\n",
    "Ytrain = VY.fit_transform([data['source'] for data in Dtrain])\n",
    "Ydev = VY.transform([data['source'] for data in Ddev])\n",
    "Ytest = VY.transform([data['source'] for data in Dtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier(loss='log')\n",
    "model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model をpickle化\n",
    "import pickle\n",
    "\n",
    "with open('SGD_loss-log_1226.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994747543205693"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain_pred=model.predict(Xtrain)\n",
    "model.score(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFZWBAY37zTQ"
   },
   "source": [
    "## 4. 検証データ上での正解率\n",
    "\n",
    "3で学習したモデルの検証データ上での正解率を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "PswMACjq7zTQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.937584803256445"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Xdev,Ydev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcmjema37zTR"
   },
   "source": [
    "## 5. 検証データ上でのマクロ平均適合率，再現率，F1スコア\n",
    "\n",
    "3で学習したモデルの検証データ上での適合率，再現率，F1スコアを求めよ．ただし，これらの指標を求めるときは，マクロ平均を用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N9kXbYOu7zTR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ydev_pred=model.predict(Xdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9360393590673417"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(Ydev, Ydev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283801510999806"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(Ydev, Ydev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.930737047396014"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Ydev, Ydev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9sqUAGs7zTR"
   },
   "source": [
    "## 6. 検証データ上での混同行列\n",
    "\n",
    "3で学習したモデルの検証データ上での混同行列を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CrhjIdsP7zTR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90,  0,  0,  1,  1,  2,  0,  0,  0],\n",
       "       [ 0, 78,  2,  0,  1,  0,  1,  2,  0],\n",
       "       [ 0,  1, 85,  0,  0,  0,  0,  1,  1],\n",
       "       [ 0,  1,  4, 39,  5,  3,  1,  1,  0],\n",
       "       [ 0,  0,  1,  0, 77,  2,  0,  1,  1],\n",
       "       [ 2,  0,  0,  3,  0, 73,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  1, 81,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 92,  3],\n",
       "       [ 0,  0,  1,  0,  0,  1,  0,  1, 76]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Ydev, Ydev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTsxjdP_7zTS"
   },
   "source": [
    "## 7. 事例の分類 (*)\n",
    "\n",
    "3で学習したモデルを用い，検証データの先頭の事例のクラスを予測し，表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dokujo-tsushin', 'it-life-hack', 'kaden-channel',\n",
       "       'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch',\n",
       "       'topic-news'], dtype='<U14')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VY.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "xa4I0jGK7zTS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 5, peachy\n",
      "true: 5, peachy\n"
     ]
    }
   ],
   "source": [
    "print(f'pred: {Ydev_pred[0]}, {VY.classes_[Ydev_pred[0]]}')\n",
    "print(f'true: {Ydev[0]}, {VY.classes_[Ydev[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbqCXM0E7zTS"
   },
   "source": [
    "## 8. クラスと確率の表示 (*)\n",
    "\n",
    "3で学習したモデルを用い，検証データの先頭の事例に対して，各クラスに分類される確率（条件付き確率）を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "E5Rd41TW7zTT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32249729e-025, 2.43165634e-186, 5.60295358e-124,\n",
       "        9.06614257e-289, 1.38627054e-226, 1.00000000e+000,\n",
       "        0.00000000e+000, 9.65786427e-278, 1.22413911e-109]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(Xdev[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPcq1vSN7zTT"
   },
   "source": [
    "## 9. 検証データをターゲットとした性能向上 (**)\n",
    "\n",
    "カテゴリ分類器のハイパーパラメータや記事からの特徴量抽出を工夫し，検証データ上でのF1スコアが最も高くなるカテゴリ分類器を見つけよ．工夫においてどのような方針でモデルを検討・実験し，その中でどのモデルの性能が最も良かったのか，説明せよ（Markdown形式で記述せよ）．さらに，検討した中で性能が最も高いカテゴリ分類器を学習するプログラムと，その分類器の評価データ上での適合率，再現率，F1スコアを報告せよ．\n",
    "\n",
    "+ 識別モデルの学習パラメータ（L2正則化の係数など）に加えて，記事の特徴量などにも工夫する余地がある．\n",
    "+ 必要であれば1.2以降の処理を変更し，単語の分かち書きの方法を変えてもよい\n",
    "+ 学習データ，検証データ，評価データの分け方を変更してはならない（1.1までの手順は変更不可）\n",
    "+ 言うまでもないが，評価データでカテゴリ分類器を学習してはいけない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は以下のような処理を行って`SGDClassifier`によるカテゴリ分類器を学習した．\n",
    "- 前処理を行う\n",
    "    - 助詞や助動詞などは無視する\n",
    "    - 数値以外は原形にする\n",
    "    - 記事本文だけでなくタイトルも用いている（タイトルを本文の先頭に結合）\n",
    "    - urlは正規表現で除去\n",
    "- TFIDFVectorizerを用いて単語をベクトル化する\n",
    "    - CountVectorizerなど様々な方法を試した中で最も良かった方法がTF-IDFによる方法だった\n",
    "    - `max_df`や`min_df`などを指定して不要な語彙を省く\n",
    "- optunaでパラメータチューニング\n",
    "    - 正則化項に関するパラメータ`alpha`と`L1_ratio`についてチューニングする\n",
    "    - 検証データを用いてチューニングする場合と，訓練データのみを用いて交差検証によるチューニングの両方を試した結果，前者のほうが良いスコアが出たためこちらを採用した\n",
    "- 最適なパラメータでのfittingを100回施行し，最も検証データのf1スコアが高かったモデルを採用する\n",
    "\n",
    "最も検証データが良かった結果は，`params = {'alpha': 4.2842570078130996e-07, 'l1_ratio': 0.0014625643848466556}`のとき\n",
    "\n",
    "|       | accuracy | precision | recall | f1_score |\n",
    "| ----- | -------- | --------- | ------ | ---------- |\n",
    "| train | 1.0      | 1.0       | 1.0    | 1.0        |\n",
    "| dev   | 0.972    | 0.971     | 0.967  | 0.968      |\n",
    "| test  | 0.970    | 0.968     | 0.965  | 0.967      |\n",
    "\n",
    "である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，混同行列は，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90,  0,  0,  0,  1,  3,  0,  0,  0],\n",
       "       [ 0, 84,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 88,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0, 47,  2,  3,  1,  0,  0],\n",
       "       [ 0,  0,  0,  0, 81,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  2, 75,  0,  0,  0],\n",
       "       [ 0,  0,  2,  0,  0,  0, 81,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 95,  0],\n",
       "       [ 0,  0,  0,  1,  0,  1,  0,  2, 75]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Ydev, Ydev_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:[0.96703297 0.98809524 0.96666667 0.93877551 0.95294118 0.90123457\n",
      " 0.97590361 0.96907216 0.96103896]\n",
      "recall_score:[0.93617021 0.98809524 0.98863636 0.85185185 0.98780488 0.93589744\n",
      " 0.97590361 0.98947368 0.93670886]\n",
      "f1_score:[0.95135135 0.98809524 0.97752809 0.89320388 0.97005988 0.91823899\n",
      " 0.97590361 0.97916667 0.94871795]\n"
     ]
    }
   ],
   "source": [
    "print(f'precision:{precision_score(Ydev, Ydev_pred, average=None)}')\n",
    "print(f'recall_score:{recall_score(Ydev, Ydev_pred, average=None)}')\n",
    "print(f'f1_score:{f1_score(Ydev, Ydev_pred, average=None)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のように，3番目（`livedoor-homme`）の再現率が低い．\n",
    "\n",
    "実際訓練データについても`livedoor-homme`の要素数が最も少ないため，学習が進まなかったのだと考えられる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 685\tdokujo-tsushin\n",
      "1: 694\tit-life-hack\n",
      "2: 697\tkaden-channel\n",
      "3: 405\tlivedoor-homme\n",
      "4: 694\tmovie-enter\n",
      "5: 687\tpeachy\n",
      "6: 712\tsmax\n",
      "7: 714\tsports-watch\n",
      "8: 614\ttopic-news\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(VY.classes_)):\n",
    "    print(f'{i}: {len(np.where(Ytrain == i)[0])}\\t{VY.classes_[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで，`class_weight='balanced'`を指定して同じように学習すると性能が上がると考えたが，以下のようにテストデータの性能は下がっている．\n",
    "\n",
    "(`{'alpha': 6.246369486906041e-07, 'l1_ratio': 0.0005504654723293009}`)\n",
    "\n",
    "|       | accuracy | precision | recall | f1_measure |\n",
    "| ----- | -------- | --------- | ------ | ---------- |\n",
    "| train | 1.000      | 1.000       | 1.000    | 1.000        |\n",
    "| dev   | 0.973    | 0.972     | 0.969  | 0.970      |\n",
    "| test  | 0.966    | 0.965     | 0.961  | 0.963      |\n",
    "\n",
    "また，混同行列は，３番め目の分類についてごく僅かに改善の傾向が見られるが，誤差の範囲である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91,  0,  0,  1,  0,  2,  0,  0,  0],\n",
       "       [ 0, 84,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 88,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  0, 48,  2,  2,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 81,  1,  0,  0,  0],\n",
       "       [ 1,  0,  0,  1,  2, 74,  0,  0,  0],\n",
       "       [ 0,  0,  2,  0,  0,  0, 81,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 94,  1],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  2, 76]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Ydev, Ydev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用いたライブラリ．　一部使用していないものもある\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import optuna\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import neologdn\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読込\n",
    "import json\n",
    "\n",
    "with open('livedoor.json') as fi:\n",
    "    D = json.load(fi)\n",
    "    \n",
    "Dtrain = D['train']\n",
    "Ddev = D['dev']\n",
    "Dtest = D['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "# 今回はあまり効果が出ないので用いていない\n",
    "ss_X = StandardScaler()\n",
    "Xtrain_ = ss_X.fit_transform(Xtrain)\n",
    "Xdev_ = ss_X.transform(Xdev)\n",
    "Xtest_ = ss_X.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "\n",
    "# mecab-ipadic-neologdを使っても性能が出なかった\n",
    "# neologd = MeCab.Tagger('-d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "# neologd.parse('') \n",
    "\n",
    "def tokenize(s):\n",
    "    return tagger.parse(s).split()\n",
    "\n",
    "def token2vec(token):\n",
    "    vec = defaultdict(int)\n",
    "    for sentence in token:\n",
    "        for word in sentence:\n",
    "                vec[word] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlre=re.compile(r'(http|https)://([-\\w]+\\.)+[-\\w]+(/[-\\w./?%&=]*)?')\n",
    "symbolre = re.compile('[，．、。]')\n",
    "# numre = re.compile(r'\\d+')\n",
    "\n",
    "def mytokenize(d):\n",
    "    s=''.join(d)\n",
    "    token = []\n",
    "    s=s.replace('\\u3000','')\n",
    "    neologdn.normalize(s)\n",
    "    s=urlre.sub(\"\", s)\n",
    "    s=symbolre.sub(\" \", s)\n",
    "#     s=numre.sub('0', s)\n",
    "    node = tagger.parseToNode(s)\n",
    "    while node:\n",
    "        features = node.feature.split(',')\n",
    "        pos = features[0]\n",
    "#         pos_sub1 = features[1]\n",
    "        base = features[6]\n",
    "        if node.surface == '':\n",
    "            node = node.next\n",
    "            continue\n",
    "        if pos in ['名詞', '動詞', '形容詞', '連体詞', '副詞', '感動詞', '記号']: # and pos_sub1 not in  ['非自立', '接尾']:\n",
    "            if base == \"*\":\n",
    "                token.append(node.surface)\n",
    "            else:\n",
    "                token.append(base)\n",
    "\n",
    "        node = node.next\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vector化 TFIDF:単語の重要度によるベクトル化． 単語の出現頻度と逆文書頻度（単語の希少さ）の積\n",
    "# min_df=3 : 出現数3未満の語彙は除外\n",
    "# max_df=0.7 : 70%の文書で出現する語彙は除外\n",
    "vectorizer = TfidfVectorizer(analyzer=mytokenize,min_df=3, max_df=0.7, norm='l2', sublinear_tf=True)\n",
    "vectorizer.fit([[d['title']]+d['text'] for d in Dtrain])\n",
    "\n",
    "# 用いるデータ\n",
    "# titleとtextを結合してベクトル化\n",
    "Xtrain_tfidf = vectorizer.transform([[d['title']]+ d['text'] for d in Dtrain])\n",
    "Xdev_tfidf =vectorizer.transform([[d['title']]+ d['text'] for d in Ddev])\n",
    "Xtest_tfidf = vectorizer.transform([[d['title']]+ d['text'] for d in Dtest])\n",
    "\n",
    "# 分類\n",
    "VY = preprocessing.LabelEncoder()\n",
    "Ytrain = VY.fit_transform([data['source'] for data in Dtrain])\n",
    "Ydev = VY.transform([data['source'] for data in Ddev])\n",
    "Ytest = VY.transform([data['source'] for data in Dtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt1(不採用)\n",
    "# 訓練データの交差検証によるチューニング\n",
    "Xtrain_opt = Xtrain_tfidf\n",
    "Xdev_opt = Xdev_tfidf\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-2)\n",
    "    l1_ratio = trial.suggest_loguniform('l1_ratio', 1e-10, 1e-2)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    clf = SGDClassifier(loss='log', alpha=alpha, l1_ratio = l1_ratio)\n",
    "    scores=cross_validate(clf, X=Xtrain_opt,y=Ytrain, scoring='f1_macro',cv=skf)\n",
    "    return scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2\n",
    "# 検証データを用いたパラメータチューニング\n",
    "Xtrain_opt = Xtrain_tfidf\n",
    "Xdev_opt = Xdev_tfidf\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-2)\n",
    "    l1_ratio = trial.suggest_loguniform('l1_ratio', 1e-10, 1)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    clf = SGDClassifier(loss='log', alpha=alpha, l1_ratio = l1_ratio)\n",
    "    clf.fit(Xtrain_opt, Ytrain)\n",
    "    Ydev_pred = clf.predict(Xdev_opt)\n",
    "    return f1_score(Ydev, Ydev_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000回のトライアルで最適なパラメータを採用\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)\n",
    "params=study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9684165110263153\n"
     ]
    }
   ],
   "source": [
    "# パラメータは固定して100回の施行で検証データのf1scoreが最大となるモデルを採用\n",
    "best_score=0\n",
    "for i in range(100):\n",
    "    clf = SGDClassifier(loss='log')\n",
    "    clf.set_params(**params)\n",
    "    clf.fit(Xtrain_tfidf, Ytrain)\n",
    "    Ydev_pred=clf.predict(Xdev_tfidf)\n",
    "    if best_score< f1_score(Ydev, Ydev_pred, average=\"macro\"):\n",
    "        best_model = clf\n",
    "        best_score = f1_score(Ydev, Ydev_pred, average=\"macro\")\n",
    "print(best_score)\n",
    "clf = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model をpickle化\n",
    "import pickle\n",
    "\n",
    "with open('SGD_best.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.003808901322441786, 'l1_ratio': 5.332697964899795e-08}\n",
      "train: 1.0\n",
      "precision:1.0\n",
      "recall_score:1.0\n",
      "f1_score:1.0\n",
      "dev: 0.9715061058344641\n",
      "precision:0.9709592982837232\n",
      "recall_score:0.9669368023926308\n",
      "f1_score:0.9684165110263153\n",
      "test: 0.9701492537313433\n",
      "test precision:0.968393445978932\n",
      "test recall_score:0.9652803286692152\n",
      "test f1_score:0.9665045737809737\n",
      "\n",
      "train: 1.0\n",
      "dev: 0.9715061058344641\n",
      "test: 0.9701492537313433\n"
     ]
    }
   ],
   "source": [
    "# 結果の出力(検証データ最大)\n",
    "Ytrain_prev=clf.predict(Xtrain_tfidf)\n",
    "Ydev_pred=clf.predict(Xdev_tfidf)\n",
    "Ytest_pred=clf.predict(Xtest_tfidf)\n",
    "print(params)\n",
    "print(f'train: {clf.score(Xtrain_tfidf, Ytrain)}')\n",
    "print(f'precision:{precision_score(Ytrain, Ytrain_prev, average=\"macro\")}')\n",
    "print(f'recall_score:{recall_score(Ytrain, Ytrain_prev, average=\"macro\")}')\n",
    "print(f'f1_score:{f1_score(Ytrain, Ytrain_prev, average=\"macro\")}')\n",
    "print(f'dev: {clf.score(Xdev_tfidf, Ydev)}')\n",
    "print(f'precision:{precision_score(Ydev, Ydev_pred, average=\"macro\")}')\n",
    "print(f'recall_score:{recall_score(Ydev, Ydev_pred, average=\"macro\")}')\n",
    "print(f'f1_score:{f1_score(Ydev, Ydev_pred, average=\"macro\")}')\n",
    "print(f'test: {clf.score(Xtest_tfidf, Ytest)}')\n",
    "print(f'test precision:{precision_score(Ytest, Ytest_pred, average=\"macro\")}')\n",
    "print(f'test recall_score:{recall_score(Ytest, Ytest_pred, average=\"macro\")}')\n",
    "print(f'test f1_score:{f1_score(Ytest, Ytest_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svdによる次元削減（不採用）\n",
    "svd=TruncatedSVD(n_components=1000)\n",
    "svd.fit(Xtrain_tfidf)\n",
    "\n",
    "Xtrain_svd=svd.transform(Xtrain_tfidf)\n",
    "Xdev_svd=svd.transform(Xdev_tfidf)\n",
    "Xtest_svd=svd.transform(Xtest_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umapによる次元削減（不採用）\n",
    "um=umap.UMAP()\n",
    "um.fit(Xtrain_tfidf)\n",
    "\n",
    "Xtrain_um=um.transform(Xtrain_tfidf)\n",
    "Xdev_um=um.transform(Xdev_tfidf)\n",
    "Xtest_um=um.transform(Xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt2\n",
    "# 検証データを用いたパラメータチューニング\n",
    "Xtrain_opt = Xtrain_tfidf\n",
    "Xdev_opt = Xdev_tfidf\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-2)\n",
    "    l1_ratio = trial.suggest_loguniform('l1_ratio', 1e-10, 1)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    clf = SGDClassifier(loss='log', alpha=alpha, l1_ratio = l1_ratio, class_weight='balanced')\n",
    "    clf.fit(Xtrain_opt, Ytrain)\n",
    "    Ydev_pred = clf.predict(Xdev_opt)\n",
    "    return f1_score(Ydev, Ydev_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000回のトライアルで最適なパラメータを採用\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)\n",
    "params=study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 6.246369486906041e-07, 'l1_ratio': 0.0005504654723293009}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9701996303242613\n"
     ]
    }
   ],
   "source": [
    "# パラメータは固定して100回の施行で検証データのf1scoreが最大となるモデルを採用\n",
    "best_score=0\n",
    "for i in range(100):\n",
    "    clf = SGDClassifier(loss='log', class_weight='balanced')\n",
    "    clf.set_params(**params)\n",
    "    clf.fit(Xtrain_tfidf, Ytrain)\n",
    "    Ydev_pred=clf.predict(Xdev_tfidf)\n",
    "    if best_score< f1_score(Ydev, Ydev_pred, average=\"macro\"):\n",
    "        best_model = clf\n",
    "        best_score = f1_score(Ydev, Ydev_pred, average=\"macro\")\n",
    "print(best_score)\n",
    "clf = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model をpickle化\n",
    "import pickle\n",
    "\n",
    "with open('SGD_best_balanced.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 6.246369486906041e-07, 'l1_ratio': 0.0005504654723293009}\n",
      "train: 0.9998305659098611\n",
      "precision:0.9998193315266486\n",
      "recall_score:0.9998439450686641\n",
      "f1_score:0.9998315099623546\n",
      "dev: 0.9728629579375848\n",
      "precision:0.9719748031952378\n",
      "recall_score:0.9689888263514056\n",
      "f1_score:0.9701996303242613\n",
      "test: 0.966078697421981\n",
      "test precision:0.9647403127029965\n",
      "test recall_score:0.9610233444545434\n",
      "test f1_score:0.9626538472838951\n"
     ]
    }
   ],
   "source": [
    "# 結果の出力(検証データ最大)\n",
    "Ytrain_prev=clf.predict(Xtrain_tfidf)\n",
    "Ydev_pred=clf.predict(Xdev_tfidf)\n",
    "Ytest_pred=clf.predict(Xtest_tfidf)\n",
    "print(params)\n",
    "print(f'train: {clf.score(Xtrain_tfidf, Ytrain)}')\n",
    "print(f'precision:{precision_score(Ytrain, Ytrain_prev, average=\"macro\")}')\n",
    "print(f'recall_score:{recall_score(Ytrain, Ytrain_prev, average=\"macro\")}')\n",
    "print(f'f1_score:{f1_score(Ytrain, Ytrain_prev, average=\"macro\")}')\n",
    "print(f'dev: {clf.score(Xdev_tfidf, Ydev)}')\n",
    "print(f'precision:{precision_score(Ydev, Ydev_pred, average=\"macro\")}')\n",
    "print(f'recall_score:{recall_score(Ydev, Ydev_pred, average=\"macro\")}')\n",
    "print(f'f1_score:{f1_score(Ydev, Ydev_pred, average=\"macro\")}')\n",
    "print(f'test: {clf.score(Xtest_tfidf, Ytest)}')\n",
    "print(f'test precision:{precision_score(Ytest, Ytest_pred, average=\"macro\")}')\n",
    "print(f'test recall_score:{recall_score(Ytest, Ytest_pred, average=\"macro\")}')\n",
    "print(f'test f1_score:{f1_score(Ytest, Ytest_pred, average=\"macro\")}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "report1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
